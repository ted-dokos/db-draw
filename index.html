<html>

<head>
	<link rel="icon" href="data:;base64,=">
	<meta charset="utf-8" />
	<script src="src/wasm_exec.js"></script>
	<script type="module" src="src/patterns.js"></script>
	<script type="module" src="src/vector.js"></script>
	<style>
		canvas {
			margin-right: auto;
			margin-left: auto;
			display: block;
		}

		div.text {
			margin-right: 30%;
			margin-left: 30%;
			display: block;
			font-family: "Open Sans", sans-serif;
			font-size: 1.3em;
		}
	</style>
</head>

<body>
	<div class="text">
		<h1>Introduction</h1>
		<p>
			I wrote this because I don't know how distributed databases work in practice.
			In the past I had "read" the bigtable paper and the Paxos paper,
			but I never felt like I understood things well.
		</p>
		<p>
			As you read through this, keep some examples of large databases in mind.
			Some examples I like are:
		</p>
		<ul>
			<li>
				The database backing a photo sharing app, like Google Photos.
				User's photos are stored in the cloud,
				and can be kept private, or shared with other users via albums.
			</li>
			<li>
				A database for banking accounts.
				Some accounts will not let you withdraw money
				if the transaction takes the account below zero.
				Protecting against race conditions becomes important.
				<!-- For example, you and your spouse might be on different continents this week.
				If you both simultaneously try to withdraw all the money from your shared account,
				one of those transactions should fail.
				If they both go through, the bank has lost money. -->
			</li>
			<li>
				Different kinds of video streaming services,
				like YouTube, Netflix, or Twitch.
			</li>
		</ul>
		<h1>Our model</h1>
		<p>
			We will start with a simple model for a database, and improve it incrementally.
			We can envision it as a single server that stores its data in memory.
			Each client has a channel of communication to the database,
			and can send read or write requests.
		</p>
	</div>
	<canvas id="draw" height="500"></canvas>
	<div class="text">
		<p>
			This database is easy to understand.
			It is also <u>consistent</u>:
			two identical read requests that arrive at the same time
			will give identical answers.
			However, it has a number of drawbacks:
		</p>
		<ol>
			<li>
				There is a single point of failure.
				If the server crashes,
				all clients are locked out until the server comes back online.
			</li>
			<li>
				A client might be unable to reach this particular server,
				intermittently or permanently.
			</li>
			<li>
				A client could have a slow connection to the server:
				a server in America will have long communication times for a client in Asia.
			</li>
			<li>
				The number of clients is limited by the server processing power.
				A huge number of clients could overwhelm the server.
			</li>
			<li>
				The database is limited by the server memory.
				A huge database might not fit in RAM, or even in disk space.
			</li>
		</ol>
		<h1>Appetizer: sharding</h1>
		<p>
			Drawbacks 3 and 5 above can be improved by <u>sharding</u> the data.
			A large dataset is broken up into manageable chunks,
			and each chunk is served independently.
		</p>
		<p>
			With sharding,
			the database can be much larger than a server's memory footprint.
			We can keep communication times short
			by strategically locating shards close to interested clients.
		</p>
	</div>
	<canvas id="draw" height="500"></canvas>
	<div class="text">
		<p>
			Sharding works well when clients are focused on small subsets of the data.
			This is often the case in consumer-targeted applications:
			you care about your photos, and your friends' photos,
			but not about strangers' photos.
		</p>
		<p>
			Sharding introduces complexity around managing data.
			If some data is needed by all users,
			we must replicate the data in each shard,
			or provide a secondary connection to where that data lives.
			Sharding also introduces complexity around routing clients properly:
			a client who cares about circle data won't be happy
			if they're connected to the square-only database.
		</p>
		<p>
			Let's see how our examples could use sharding:
		</p>
		<ul>
			<li>
				<b>Photo sharing app:</b>
				we can store all of a user's owned photos on the same shard,
				unless they have a huge amount of photos.
				We have many possible strategies for implementing shared albums.
				One simple approach would be to store references to the original images.
				This saves on data storage,
				but means that shared album content
				could be spread across multiple shards.
			</li>
			<li>
				<b>Bank accounts:</b> accounts rarely have more than a few owners,
				and a user rarely owns more than a few accounts.
				Grouping connected owners and accounts together
				should therefore result in small clusters of related data,
				a good fit for sharding.
			<li>
				<b>Streaming services:</b> videos are memory hungry,
				but each client is only watching one video at a time.
				Sharding on a per-video basis makes a lot of sense.
			</li>
		</ul>
		<p>
			Sharding data is a useful trick in the design of systems.
			It can be applied in conjunction with replication, the next subject.
		</p>
		<h1>Main entree: replication</h1>
		<p>
			Drawbacks 1, 2, and 4 above can all be improved
			by <u>replicating</u> the data across multiple servers:
			<ol>
				<li>
					No more single point of failure.
					If a replica crashes, clients can connect to a different replica
					and continue interacting with the database.
				</li>
				<li>
					A client can access the database as long as one replica is reachable.
					<!--Under normal conditions,
					the client can use the "closest" replica,
					meaning the one with the shortest travel time for communication.-->
				</li>
				<li value="4">
					Client traffic can be split among replicas.
					A load balancer can direct clients
					and prevent replicas from becoming overwhelmed.
				</li>
			</ol>
		</p>
	</div>
	<canvas id="draw" height="350"></canvas>
	<div class="text">
		<p>
			However, replication is not a simple win.	
			In the above example, every client is only sending read requests.
			Requests that change the state of the database introduce a challenge:
			how do we keep the replicas in sync?
		</p>
	</div>
	<canvas id="draw" height="350"></canvas>
	<div class="text">
		<h3>Idea 1: decentralized messaging</h2>
		<p>
			We could accept writes from every replica.
			When a replica receives a change,
			it notifies the others, and makes them update as well.
			This idea is alluring,
			as it's very simple conceptually.
		</p>
		<p>
			Unfortunately, that simplicity comes at a cost.
			Simultaneous writes to different replicas are enough to cause desync.
			That doesn't mean this idea is dead, it just needs to be improved.
			For now, we'll drop it and look at another option.
		</p>
	</div>
	<canvas id="draw" height="350"></canvas>
	<div class="text">
		<h3>Idea 2: the leader model</h3>
		<p>
			Another approach involves selecting a replica to be the <u>leader</u>.
			All read and write requests happen via the leader,
			and the leader tells the other replicas how to update.

		</p>
	</div>
	<canvas id="draw" height="700"></canvas>
	<div class="text">
		<p>
			Lorem ipsum
		</p>
		<h1>Resources</h1>
		<ul>
			<li>
				Sharding
				<ul>
					<li>
						<a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)">Wikipedia</a>
					</li>
					<li>
						<a href="https://aws.amazon.com/what-is/database-sharding/">
							AWS: What is database sharding?
						</a>
					</li>
				</ul>
			</li>
		</ul>
	</div>
</body>
<script type="module" src="src/display.js"></script>

</html>